{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link Analysis is frequently used to visualize the relationships between nodes in a graph. In this case, we study the hyperlinks between domains that contain a certain term on the Web."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%AddDeps com.lucidworks.spark spark-solr 3.6.0 --transitive\n",
    "%AddDeps com.google.guava guava 15.0 --transitive\n",
    "%AddDeps org.jsoup jsoup 1.11.3 --transitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys.process._\n",
    "\n",
    "\"pip install matplotlib\" !\n",
    "\n",
    "\"pip install networkx\" !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Solr\n",
    "\n",
    "First we extract links referenced by websites in the ClueWeb09b collection that contain the word \"jaguar\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import com.lucidworks.spark.rdd.SelectSolrRDD\n",
    "import com.google.common.net.InternetDomainName\n",
    "import org.jsoup.Jsoup\n",
    "import org.apache.hadoop.fs.{FileSystem, Path}\n",
    "\n",
    "import scala.collection.JavaConverters._\n",
    "import java.net.URL\n",
    "\n",
    "// Solr's ZooKeeper URL\n",
    "val SOLR = \"192.168.1.111:9983\"\n",
    "\n",
    "// The Solr collection\n",
    "val INDEX = \"cw09b-url\"\n",
    "\n",
    "// The Solr query\n",
    "val QUERY = \"contents:jaguar\"\n",
    "\n",
    "// The number of partitions\n",
    "val PARTITIONS = 8\n",
    "\n",
    "// The limit for number of rows to process\n",
    "val LIMIT = 1000\n",
    "\n",
    "// The output path\n",
    "val OUT_DIR = \"link_analysis\"\n",
    "\n",
    "// Delete old output dir\n",
    "FileSystem.get(sc.hadoopConfiguration).delete(new Path(OUT_DIR), true)\n",
    "\n",
    "val source_urls = new SelectSolrRDD(SOLR, INDEX, sc, maxRows = Some(LIMIT))\n",
    ".rows(100)\n",
    ".query(QUERY)\n",
    ".repartition(PARTITIONS)\n",
    ".mapPartitions(docs => {\n",
    "    docs.map(doc => {\n",
    "        val url = doc.get(\"url\") + \"\"\n",
    "        (InternetDomainName.from(new URL(url.substring(1, url.length - 1)).getHost).topPrivateDomain().name(), doc.get(\"raw\") + \"\")\n",
    "    })\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Links\n",
    "\n",
    "We then randomly sample 1% of the retrieved documents and extract the top three most frequently-occurring outgoing links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val zipped_urls = source_urls.sample(withReplacement=false, fraction=0.01, seed=42)\n",
    ".flatMap(record => {\n",
    "        val target_urls = Jsoup.parse(record._2)\n",
    "          .select(\"a[href]\")\n",
    "          .asScala\n",
    "          .map(link => link.attr(\"abs:href\"))\n",
    "          .filter(!_.isEmpty)\n",
    "          .map(link => {\n",
    "            try { InternetDomainName.from(new URL(link).getHost).topPrivateDomain().name() }\n",
    "            catch {\n",
    "              case e: Exception => println(\"\")\n",
    "                \"\"\n",
    "            }\n",
    "          })\n",
    "          .distinct\n",
    "          .take(3)\n",
    "        val src_host = (1 to target_urls.size).map(_ => record._1)\n",
    "        src_host zip target_urls\n",
    "      })\n",
    "      .distinct\n",
    "      .filter(x => x._1 != x._2)\n",
    "      .map(pair => pair._1 + \";\" + pair._2)\n",
    "      .coalesce(1)\n",
    "\n",
    "zipped_urls.saveAsTextFile(OUT_DIR)\n",
    "zipped_urls.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Network Graph\n",
    "\n",
    "The output contains a list of semi-column separated domain pairs.\n",
    "You may directly feed this file into your favorite visualization tool to create a network graph.\n",
    "We use Gephi with Multilevel Layout in our paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys.process._\n",
    "\n",
    "// Remove the old output directory\n",
    "\"rm -rf /tmp/link_analysis\" !\n",
    "\n",
    "// Copy new output from HDFS to local filesystem\n",
    "\"hdfs dfs -copyToLocal link_analysis /tmp/link_analysis\" !\n",
    "\n",
    "\"python draw_graph.py\" !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](/tmp/link_analysis/network_graph.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
