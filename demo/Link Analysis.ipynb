{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link Analysis is frequently used to visualize the relationships between nodes in a graph. In this case, we study the hyperlinks between domains that contain a certain term on the Web."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%init_spark\n",
    "launcher.conf.spark.executor.instances = 8\n",
    "launcher.conf.spark.executor.cores = 32\n",
    "launcher.conf.spark.executor.memory = '8G'\n",
    "launcher.conf.spark.driver.memory = '4G'\n",
    "launcher.jars = [\"sparksolrini.jar\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://desktop:4040\n",
       "SparkContext available as 'sc' (version = 2.4.0, master = local[*], app id = local-1563129268111)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /home/ryan/.conda/envs/sparksolrini/lib/python3.7/site-packages (3.1.1)\n",
      "Requirement already satisfied: numpy>=1.11 in /home/ryan/.local/lib/python3.7/site-packages (from matplotlib) (1.16.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ryan/.conda/envs/sparksolrini/lib/python3.7/site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/ryan/.conda/envs/sparksolrini/lib/python3.7/site-packages (from matplotlib) (2.8.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ryan/.conda/envs/sparksolrini/lib/python3.7/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/ryan/.conda/envs/sparksolrini/lib/python3.7/site-packages (from matplotlib) (2.4.0)\n",
      "Requirement already satisfied: setuptools in /home/ryan/.conda/envs/sparksolrini/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib) (41.0.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/ryan/.conda/envs/sparksolrini/lib/python3.7/site-packages (from python-dateutil>=2.1->matplotlib) (1.12.0)\n",
      "Requirement already satisfied: networkx in /home/ryan/.conda/envs/sparksolrini/lib/python3.7/site-packages (2.3)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /home/ryan/.conda/envs/sparksolrini/lib/python3.7/site-packages (from networkx) (4.4.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "warning: there were two feature warnings; re-run with -feature for details\n",
       "import sys.process._\n",
       "res0: Int = 0\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys.process._\n",
    "\n",
    "\"pip install matplotlib\" !\n",
    "\n",
    "\"pip install networkx\" !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Solr\n",
    "\n",
    "First we extract links referenced by websites in the ClueWeb09b collection that contain the word \"jaguar\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import com.lucidworks.spark.rdd.SelectSolrRDD\n",
       "import com.google.common.net.InternetDomainName\n",
       "import org.jsoup.Jsoup\n",
       "import org.apache.hadoop.fs.{FileSystem, Path}\n",
       "import scala.collection.JavaConverters._\n",
       "import java.net.URL\n",
       "SOLR: String = 192.168.1.111:9983\n",
       "INDEX: String = cw09b-url\n",
       "QUERY: String = contents:jaguar\n",
       "PARTITIONS: Int = 8\n",
       "LIMIT: Int = 10000\n",
       "source_urls: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[9] at mapPartitions at <console>:55\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import com.lucidworks.spark.rdd.SelectSolrRDD\n",
    "import com.google.common.net.InternetDomainName\n",
    "import org.jsoup.Jsoup\n",
    "import org.apache.hadoop.fs.{FileSystem, Path}\n",
    "\n",
    "import scala.collection.JavaConverters._\n",
    "import java.net.URL\n",
    "\n",
    "// Solr's ZooKeeper URL\n",
    "val SOLR = \"192.168.1.111:9983\"\n",
    "\n",
    "// The Solr collection\n",
    "val INDEX = \"cw09b-url\"\n",
    "\n",
    "// The Solr query\n",
    "val QUERY = \"contents:jaguar\"\n",
    "\n",
    "// The number of partitions\n",
    "val PARTITIONS = 8\n",
    "\n",
    "// The limit for number of rows to process\n",
    "val LIMIT = 10000\n",
    "\n",
    "val source_urls = new SelectSolrRDD(SOLR, INDEX, sc, maxRows = Some(LIMIT))\n",
    ".rows(10000)\n",
    ".query(QUERY)\n",
    ".repartition(PARTITIONS)\n",
    ".mapPartitions(docs => {\n",
    "    docs.map(doc => {\n",
    "        val url = doc.get(\"url\") + \"\"\n",
    "        try { (InternetDomainName.from(new URL(url.substring(1, url.length - 1)).getHost).topPrivateDomain().name(), doc.get(\"raw\") + \"\") }\n",
    "        catch {\n",
    "            case e: Exception => println(\"\")\n",
    "            (\"\", \"\")\n",
    "        }\n",
    "    })\n",
    "    .filter(!_._2.isEmpty)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Links\n",
    "\n",
    "We then randomly sample 1% of the retrieved documents and extract the top three most frequently-occurring outgoing links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OUT_DIR: String = link_analysis\n",
       "zipped_urls: org.apache.spark.rdd.RDD[String] = CoalescedRDD[17] at coalesce at <console>:66\n",
       "res1: Array[String] = Array(wikipedia.org;49ers.com)\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// The output path\n",
    "val OUT_DIR = \"link_analysis\"\n",
    "\n",
    "// Delete old output dir\n",
    "FileSystem.get(sc.hadoopConfiguration).delete(new Path(OUT_DIR), true)\n",
    "\n",
    "val zipped_urls = source_urls.sample(withReplacement=false, fraction=0.01, seed=42)\n",
    ".flatMap(record => {\n",
    "        val target_urls = Jsoup.parse(record._2)\n",
    "          .select(\"a[href]\")\n",
    "          .asScala\n",
    "          .map(link => link.attr(\"abs:href\"))\n",
    "          .filter(!_.isEmpty)\n",
    "          .map(link => {\n",
    "            try { InternetDomainName.from(new URL(link).getHost).topPrivateDomain().name() }\n",
    "            catch {\n",
    "              case e: Exception => println(\"\")\n",
    "                \"\"\n",
    "            }\n",
    "          })\n",
    "          .distinct\n",
    "          .take(3)\n",
    "        val src_host = (1 to target_urls.size).map(_ => record._1)\n",
    "        src_host zip target_urls\n",
    "      })\n",
    "      .distinct\n",
    "      .filter(x => x._1 != x._2)\n",
    "      .map(pair => pair._1 + \";\" + pair._2)\n",
    "      .coalesce(1)\n",
    "\n",
    "zipped_urls.saveAsTextFile(OUT_DIR)\n",
    "zipped_urls.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Network Graph\n",
    "\n",
    "The output contains a list of semi-column separated domain pairs.\n",
    "You may directly feed this file into your favorite visualization tool to create a network graph.\n",
    "We use Gephi with Multilevel Layout in our paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "warning",
     "evalue": " there were three feature warnings; re-run with -feature for details",
     "output_type": "error",
     "traceback": [
      "warning: there were three feature warnings; re-run with -feature for details",
      "java.io.IOException: Cannot run program \"hdfs\": error=2, No such file or directory",
      "  at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)",
      "  at scala.sys.process.ProcessBuilderImpl$Simple.run(ProcessBuilderImpl.scala:69)",
      "  at scala.sys.process.ProcessBuilderImpl$AbstractBuilder.run(ProcessBuilderImpl.scala:98)",
      "  at scala.sys.process.ProcessBuilderImpl$AbstractBuilder.$bang(ProcessBuilderImpl.scala:112)",
      "  ... 41 elided",
      "Caused by: java.io.IOException: error=2, No such file or directory",
      "  at java.lang.UNIXProcess.forkAndExec(Native Method)",
      "  at java.lang.UNIXProcess.<init>(UNIXProcess.java:247)",
      "  at java.lang.ProcessImpl.start(ProcessImpl.java:134)",
      "  at java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)",
      "  ... 44 more",
      ""
     ]
    }
   ],
   "source": [
    "import sys.process._\n",
    "\n",
    "// Remove the old output directory\n",
    "\"rm -rf network_graph.png /tmp/link_analysis\" !\n",
    "\n",
    "// Copy new output from HDFS to local filesystem\n",
    "\"hdfs dfs -copyToLocal link_analysis /tmp/link_analysis\" !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "g = nx.read_edgelist('/tmp/link_analysis/part-00000', delimiter=';', create_using=nx.Graph(), nodetype=str)\n",
    "\n",
    "print(nx.info(g))\n",
    "\n",
    "nx.draw_networkx(g, arrows=True, node_size=20, with_labels=False)\n",
    "\n",
    "plt.show()\n",
    "plt.savefig('network_graph.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](network_graph.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
