{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel Density Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernel Density Estimate (KDE) is a statistical technique to estimate the probability density function of a random variable.\n",
    "In this notebook, we employ KDE to visualize the distribution of tweets that contain a certan term over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marking com.lucidworks.spark:spark-solr:3.6.0 for download\n",
      "-> Failed to resolve org.restlet.jee:org.restlet:2.3.0\n",
      "    -> not found: /tmp/toree-tmp-dir1787466831314273765/toree_add_deps/cache/org.restlet.jee/org.restlet/ivy-2.3.0.xml\n",
      "    -> not found: https://repo1.maven.org/maven2/org/restlet/jee/org.restlet/2.3.0/org.restlet-2.3.0.pom\n",
      "-> Failed to resolve org.restlet.jee:org.restlet.ext.servlet:2.3.0\n",
      "    -> not found: /tmp/toree-tmp-dir1787466831314273765/toree_add_deps/cache/org.restlet.jee/org.restlet.ext.servlet/ivy-2.3.0.xml\n",
      "    -> not found: https://repo1.maven.org/maven2/org/restlet/jee/org.restlet.ext.servlet/2.3.0/org.restlet.ext.servlet-2.3.0.pom\n",
      "Obtained 388 files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%AddDeps com.lucidworks.spark spark-solr 3.6.0 --transitive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Solr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "updateTime: (hour: Int, createdAt: String)Int\n",
       "timeZoneToInt: (timeZone: String)Int\n",
       "shiftHours: (hour: Int, shift: Int)Int\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def updateTime(hour:Int, createdAt:String):Int = {\n",
    "    var adjusted = hour\n",
    "    createdAt match {\n",
    "      case \"Pacific Time (US & Canada)\" => adjusted = shiftHours(hour, -8)\n",
    "      case \"Eastern Time (US & Canada)\" => adjusted = shiftHours(hour, -5)\n",
    "      case \"Central Time (US & Canada)\" => adjusted = shiftHours(hour, -5)\n",
    "      case \"Mountain Time (US & Canada)\" => adjusted = shiftHours(hour, -6)\n",
    "      case \"Atlantic Time (Canada)\" => adjusted = shiftHours(hour, -4)\n",
    "    }\n",
    "    adjusted\n",
    "}\n",
    "\n",
    "def timeZoneToInt(timeZone:String):Int = {\n",
    "    var out = 6 // sunday\n",
    "\n",
    "    if (timeZone contains \"Mon\") {\n",
    "      out = 0\n",
    "    } else if (timeZone contains \"Tue\") {\n",
    "      out = 1\n",
    "    } else if (timeZone contains \"Wed\") {\n",
    "      out = 2\n",
    "    } else if (timeZone contains \"Thu\") {\n",
    "      out = 3\n",
    "    } else if (timeZone contains \"Fri\") {\n",
    "      out = 4\n",
    "    } else if (timeZone contains \"Sat\") {\n",
    "      out = 5\n",
    "    }\n",
    "    out\n",
    "}\n",
    "\n",
    "def shiftHours(hour:Int, shift:Int):Int = {\n",
    "    var adjusted = hour + shift\n",
    "    if (adjusted >= 24) {\n",
    "      adjusted %= 24\n",
    "    } else if (adjusted < 0) {\n",
    "      adjusted += 24\n",
    "    }\n",
    "    adjusted\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we find the tweets that contain the term TERM, which are created in Canada or USA. We accumulate the tweets over a certain time period, MODE (e.g: day or hour)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SOLR = 192.168.1.111:9983\n",
       "INDEX = mb13\n",
       "MODE = day\n",
       "TERM = church\n",
       "QUERY = contents:$TERM\n",
       "timeRegex = ([0-9]+):([0-9]+):([0-9]+)\n",
       "rdd = MapPartitionsRDD[13] at flatMap at <console>:74\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[13] at flatMap at <console>:74"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import com.lucidworks.spark.rdd.SelectSolrRDD\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "import org.apache.spark.mllib.stat.KernelDensity\n",
    "import play.api.libs.json._\n",
    "\n",
    "// Solr's ZooKeeper URL\n",
    "val SOLR = \"192.168.1.111:9983\"\n",
    "\n",
    "// The Solr collection\n",
    "val INDEX = \"mb13\"\n",
    "\n",
    "// The Solr query\n",
    "val MODE = \"day\"  // day OR hour\n",
    "val TERM = \"church\"\n",
    "val QUERY = \"contents:$TERM\"\n",
    "\n",
    "val timeRegex = raw\"([0-9]+):([0-9]+):([0-9]+)\".r\n",
    "\n",
    "val rdd = new SelectSolrRDD(SOLR, INDEX, sc)\n",
    ".rows(10000)\n",
    ".query(QUERY)\n",
    ".flatMap(doc => {\n",
    "    val parsedJson = Json.parse(doc.get(\"raw\").toString)\n",
    "    var out:List[Tuple3[Int, Double, Int]] = List()\n",
    "\n",
    "    try {\n",
    "        val timeZone:String = (parsedJson \\ \"user\" \\ \"time_zone\").as[String]\n",
    "        if ((timeZone contains \"Canada\") || (timeZone contains \"US\")) {\n",
    "            val time = (parsedJson \\ \"created_at\").as[String]\n",
    "            val matches = timeRegex.findFirstMatchIn(time)\n",
    "            val hour = updateTime(matches.get.group(1).toInt, timeZone)\n",
    "            val week = timeZoneToInt(time)\n",
    "            val min = matches.get.group(2).toDouble\n",
    "            out = if (MODE == \"day\") List((week, hour/24, 1)) else List((hour, min/60, 1))        \n",
    "            }\n",
    "        } catch {\n",
    "          case e : Exception => println(\"unable to parse the tweet\", e)\n",
    "        }\n",
    "        out\n",
    "      }).persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute KDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts / density per day for church\n",
      "0 ( 3629101 ) -- 0.09827479434429007\n",
      "1 ( 3586485 ) -- 0.13149417467469857\n",
      "2 ( 3553899 ) -- 0.1386817583600064\n",
      "3 ( 3517636 ) -- 0.14163367121194861\n",
      "4 ( 3921588 ) -- 0.14477533633158732\n",
      "5 ( 3673112 ) -- 0.13782699122788605\n",
      "6 ( 3797445 ) -- 0.10247753456187696\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "counts = Map(0 -> 3629101, 5 -> 3673112, 1 -> 3586485, 6 -> 3797445, 2 -> 3553899, 3 -> 3517636, 4 -> 3921588)\n",
       "kdeData = MapPartitionsRDD[19] at map at <console>:60\n",
       "kd = org.apache.spark.mllib.stat.KernelDensity@18288e06\n",
       "domain = Array(0, 1, 2, 3, 4, 5, 6)\n",
       "densities = Array(0.09827479434429007, 0.13149417467469857, 0.1386817583600064, 0.14163367121194861, 0.14477533633158732, 0.13782699122788605, 0.10247753456187696)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array(0.09827479434429007, 0.13149417467469857, 0.1386817583600064, 0.14163367121194861, 0.14477533633158732, 0.13782699122788605, 0.10247753456187696)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val counts = rdd.map(item => (item._1, item._3)).reduceByKey(_+_).sortByKey().collect().toMap\n",
    "\n",
    "val kdeData = rdd.map(item => item._1.toInt.toDouble + item._2)\n",
    "\n",
    "val kd = if (MODE == \"day\") new KernelDensity().setSample(kdeData).setBandwidth(1.0) else new KernelDensity().setSample(kdeData).setBandwidth(2.0)\n",
    "val domain = if (MODE ==  \"day\") (0 to 6).toArray else (0 to 23).toArray\n",
    "\n",
    "val densities = kd.estimate(domain.map(_.toDouble))\n",
    "\n",
    "println(s\"counts / density per $MODE for $TERM\")\n",
    "domain.foreach(x => {\n",
    "    println(s\"$x ( ${counts(x)} ) -- ${densities(x)}\")\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
