{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel Density Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernel Density Estimate (KDE) is a statistical technique to estimate the probability density function of a random variable.\n",
    "In this notebook, we employ KDE to visualize the distribution of tweets that contain a certan term over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%AddDeps com.lucidworks.spark spark-solr 3.6.0 --transitive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Solr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "updateTime: (hour: Int, createdAt: String)Int\n",
       "timeZoneToInt: (timeZone: String)Int\n",
       "shiftHours: (hour: Int, shift: Int)Int\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def updateTime(hour:Int, createdAt:String):Int = {\n",
    "    var adjusted = hour\n",
    "    createdAt match {\n",
    "      case \"Pacific Time (US & Canada)\" => adjusted = shiftHours(hour, -8)\n",
    "      case \"Eastern Time (US & Canada)\" => adjusted = shiftHours(hour, -5)\n",
    "      case \"Central Time (US & Canada)\" => adjusted = shiftHours(hour, -5)\n",
    "      case \"Mountain Time (US & Canada)\" => adjusted = shiftHours(hour, -6)\n",
    "      case \"Atlantic Time (Canada)\" => adjusted = shiftHours(hour, -4)\n",
    "    }\n",
    "    adjusted\n",
    "}\n",
    "\n",
    "def timeZoneToInt(timeZone:String):Int = {\n",
    "    var out = 6 // sunday\n",
    "\n",
    "    if (timeZone contains \"Mon\") {\n",
    "      out = 0\n",
    "    } else if (timeZone contains \"Tue\") {\n",
    "      out = 1\n",
    "    } else if (timeZone contains \"Wed\") {\n",
    "      out = 2\n",
    "    } else if (timeZone contains \"Thu\") {\n",
    "      out = 3\n",
    "    } else if (timeZone contains \"Fri\") {\n",
    "      out = 4\n",
    "    } else if (timeZone contains \"Sat\") {\n",
    "      out = 5\n",
    "    }\n",
    "    out\n",
    "}\n",
    "\n",
    "def shiftHours(hour:Int, shift:Int):Int = {\n",
    "    var adjusted = hour + shift\n",
    "    if (adjusted >= 24) {\n",
    "      adjusted %= 24\n",
    "    } else if (adjusted < 0) {\n",
    "      adjusted += 24\n",
    "    }\n",
    "    adjusted\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we find the tweets that contain the term TERM, which are created in Canada or USA. We accumulate the tweets over a certain time period, MODE (e.g: day or hour)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SOLR = 192.168.1.111:9983\n",
       "INDEX = mb13\n",
       "MODE = day\n",
       "TERM = school\n",
       "QUERY = contents:school\n",
       "LIMIT = 1000\n",
       "OUT_DIR = kde\n",
       "timeRegex = ([0-9]+):([0-9]+):([0-9]+)\n",
       "rdd = MapPartitionsRDD[146] at flatMap at <console>:220\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[146] at flatMap at <console>:220"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import com.lucidworks.spark.rdd.SelectSolrRDD\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "import org.apache.spark.mllib.stat.KernelDensity\n",
    "import org.apache.hadoop.fs.{FileSystem, Path}\n",
    "import play.api.libs.json._\n",
    "\n",
    "// Solr's ZooKeeper URL\n",
    "val SOLR = \"192.168.1.111:9983\"\n",
    "\n",
    "// The Solr collection\n",
    "val INDEX = \"mb13\"\n",
    "\n",
    "// The Solr query\n",
    "val MODE = \"day\"  // day OR hour\n",
    "val TERM = \"school\"\n",
    "val QUERY = s\"contents:${TERM}\"\n",
    "\n",
    "// The limit for number of rows to process\n",
    "val LIMIT = 1000\n",
    "\n",
    "// Output directory\n",
    "val OUT_DIR = \"kde\"\n",
    "\n",
    "// Delete old output dir\n",
    "FileSystem.get(sc.hadoopConfiguration).delete(new Path(OUT_DIR), true)\n",
    "\n",
    "val timeRegex = raw\"([0-9]+):([0-9]+):([0-9]+)\".r\n",
    "\n",
    "val rdd = new SelectSolrRDD(SOLR, INDEX, sc, maxRows = Some(LIMIT))\n",
    ".rows(1000)\n",
    ".query(QUERY)\n",
    ".flatMap(doc => {\n",
    "    val parsedJson = Json.parse(doc.get(\"raw\").toString)\n",
    "    var out:List[Tuple3[Int, Double, Int]] = List()\n",
    "\n",
    "    try {\n",
    "        val timeZone:String = (parsedJson \\ \"user\" \\ \"time_zone\").as[String]\n",
    "        if ((timeZone contains \"Canada\") || (timeZone contains \"US\")) {\n",
    "            val time = (parsedJson \\ \"created_at\").as[String]\n",
    "            val matches = timeRegex.findFirstMatchIn(time)\n",
    "            val hour = updateTime(matches.get.group(1).toInt, timeZone)\n",
    "            val week = timeZoneToInt(time)\n",
    "            val min = matches.get.group(2).toDouble\n",
    "            out = if (MODE == \"day\") List((week, hour/24, 1)) else List((hour, min/60, 1))        \n",
    "            }\n",
    "        } catch {\n",
    "          case e : Exception => println(\"unable to parse the tweet\", e)\n",
    "        }\n",
    "        out\n",
    "      }).persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute KDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts / density per day for school\n",
      "0 ( 82 ) -- 0.14522323398019227\n",
      "1 ( 79 ) -- 0.18434542720787753\n",
      "2 ( 69 ) -- 0.17010762720068012\n",
      "3 ( 46 ) -- 0.1419207838156945\n",
      "4 ( 56 ) -- 0.11539841926988921\n",
      "5 ( 21 ) -- 0.08393051411578506\n",
      "6 ( 31 ) -- 0.053867987743947854\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "counts = Map(0 -> 82, 5 -> 21, 1 -> 79, 6 -> 31, 2 -> 69, 3 -> 46, 4 -> 56)\n",
       "kdeData = MapPartitionsRDD[150] at map at <console>:197\n",
       "kd = org.apache.spark.mllib.stat.KernelDensity@378acfaf\n",
       "domain = Array(0, 1, 2, 3, 4, 5, 6)\n",
       "densities = Array(0.14522323398019227, 0.18434542720787753, 0.17010762720068012, 0.1419207838156945, 0.11539841926988921, 0.08393051411578506, 0.053867987743947854)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array(0.14522323398019227, 0.18434542720787753, 0.17010762720068012, 0.1419207838156945, 0.11539841926988921, 0.08393051411578506, 0.053867987743947854)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val counts = rdd.map(item => (item._1, item._3)).reduceByKey(_+_).sortByKey().collect().toMap\n",
    "\n",
    "val kdeData = rdd.map(item => item._1.toInt.toDouble + item._2)\n",
    "\n",
    "val kd = if (MODE == \"day\") new KernelDensity().setSample(kdeData).setBandwidth(1.0) else new KernelDensity().setSample(kdeData).setBandwidth(2.0)\n",
    "val domain = if (MODE ==  \"day\") (0 to 6).toArray else (0 to 23).toArray\n",
    "\n",
    "val densities = kd.estimate(domain.map(_.toDouble))\n",
    "\n",
    "println(s\"counts / density per $MODE for $TERM\")\n",
    "domain.foreach(x => {\n",
    "    println(s\"$x ( ${counts(x)} ) -- ${densities(x)}\")\n",
    "})\n",
    "\n",
    "sc.parallelize(densities).coalesce(1).saveAsTextFile(OUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n",
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "warning: there were four feature warnings; re-run with -feature for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys.process._\n",
    "\n",
    "// Remove the old output directory\n",
    "\"rm -rf kde.png /tmp/kde\" !\n",
    "\n",
    "// Copy new output from HDFS to local filesystem\n",
    "\"hdfs dfs -copyToLocal kde /tmp/kde\" !\n",
    "\n",
    "// Generate the word cloud\n",
    "if (MODE == \"day\") {\n",
    "    \"python kde_day.py\" !\n",
    "} else {\n",
    "    \"python kde_hour.py\" !\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](kde.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
