{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel Density Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernel Density Estimate (KDE) is a statistical technique to estimate the probability density function of a random variable.\n",
    "In this notebook, we employ KDE to visualize the distribution of tweets that contain a certan term over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%AddDeps com.lucidworks.spark spark-solr 3.6.0 --transitive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Solr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def updateTime(hour:Int, createdAt:String):Int = {\n",
    "    var adjusted = hour\n",
    "    createdAt match {\n",
    "      case \"Pacific Time (US & Canada)\" => adjusted = shiftHours(hour, -8)\n",
    "      case \"Eastern Time (US & Canada)\" => adjusted = shiftHours(hour, -5)\n",
    "      case \"Central Time (US & Canada)\" => adjusted = shiftHours(hour, -5)\n",
    "      case \"Mountain Time (US & Canada)\" => adjusted = shiftHours(hour, -6)\n",
    "      case \"Atlantic Time (Canada)\" => adjusted = shiftHours(hour, -4)\n",
    "    }\n",
    "    adjusted\n",
    "}\n",
    "\n",
    "def timeZoneToInt(timeZone:String):Int = {\n",
    "    var out = 6 // sunday\n",
    "\n",
    "    if (timeZone contains \"Mon\") {\n",
    "      out = 0\n",
    "    } else if (timeZone contains \"Tue\") {\n",
    "      out = 1\n",
    "    } else if (timeZone contains \"Wed\") {\n",
    "      out = 2\n",
    "    } else if (timeZone contains \"Thu\") {\n",
    "      out = 3\n",
    "    } else if (timeZone contains \"Fri\") {\n",
    "      out = 4\n",
    "    } else if (timeZone contains \"Sat\") {\n",
    "      out = 5\n",
    "    }\n",
    "    out\n",
    "}\n",
    "\n",
    "def shiftHours(hour:Int, shift:Int):Int = {\n",
    "    var adjusted = hour + shift\n",
    "    if (adjusted >= 24) {\n",
    "      adjusted %= 24\n",
    "    } else if (adjusted < 0) {\n",
    "      adjusted += 24\n",
    "    }\n",
    "    adjusted\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we find the tweets that contain the term TERM, which are created in Canada or USA. We accumulate the tweets over a certain time period, MODE (e.g: day or hour)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import com.lucidworks.spark.rdd.SelectSolrRDD\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "import org.apache.spark.mllib.stat.KernelDensity\n",
    "import org.apache.hadoop.fs.{FileSystem, Path}\n",
    "import play.api.libs.json._\n",
    "\n",
    "// Solr's ZooKeeper URL\n",
    "val SOLR = \"192.168.1.111:9983\"\n",
    "\n",
    "// The Solr collection\n",
    "val INDEX = \"mb13\"\n",
    "\n",
    "// The Solr query\n",
    "val MODE = \"day\"  // day OR hour\n",
    "val TERM = \"church\"\n",
    "val QUERY = \"contents:$TERM\"\n",
    "\n",
    "// The limit for number of rows to process\n",
    "val LIMIT = 1000\n",
    "\n",
    "// Output directory\n",
    "val OUT_DIR = \"kde\"\n",
    "\n",
    "// Delete old output dir\n",
    "FileSystem.get(sc.hadoopConfiguration).delete(new Path(OUT_DIR), true)\n",
    "\n",
    "val timeRegex = raw\"([0-9]+):([0-9]+):([0-9]+)\".r\n",
    "\n",
    "val rdd = new SelectSolrRDD(SOLR, INDEX, sc, maxRows = Some(LIMIT))\n",
    ".rows(1000)\n",
    ".query(QUERY)\n",
    ".flatMap(doc => {\n",
    "    val parsedJson = Json.parse(doc.get(\"raw\").toString)\n",
    "    var out:List[Tuple3[Int, Double, Int]] = List()\n",
    "\n",
    "    try {\n",
    "        val timeZone:String = (parsedJson \\ \"user\" \\ \"time_zone\").as[String]\n",
    "        if ((timeZone contains \"Canada\") || (timeZone contains \"US\")) {\n",
    "            val time = (parsedJson \\ \"created_at\").as[String]\n",
    "            val matches = timeRegex.findFirstMatchIn(time)\n",
    "            val hour = updateTime(matches.get.group(1).toInt, timeZone)\n",
    "            val week = timeZoneToInt(time)\n",
    "            val min = matches.get.group(2).toDouble\n",
    "            out = if (MODE == \"day\") List((week, hour/24, 1)) else List((hour, min/60, 1))        \n",
    "            }\n",
    "        } catch {\n",
    "          case e : Exception => println(\"unable to parse the tweet\", e)\n",
    "        }\n",
    "        out\n",
    "      }).persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute KDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val counts = rdd.map(item => (item._1, item._3)).reduceByKey(_+_).sortByKey().collect().toMap\n",
    "\n",
    "val kdeData = rdd.map(item => item._1.toInt.toDouble + item._2)\n",
    "\n",
    "val kd = if (MODE == \"day\") new KernelDensity().setSample(kdeData).setBandwidth(1.0) else new KernelDensity().setSample(kdeData).setBandwidth(2.0)\n",
    "val domain = if (MODE ==  \"day\") (0 to 6).toArray else (0 to 23).toArray\n",
    "\n",
    "val densities = kd.estimate(domain.map(_.toDouble))\n",
    "\n",
    "println(s\"counts / density per $MODE for $TERM\")\n",
    "domain.foreach(x => {\n",
    "    println(s\"$x ( ${counts(x)} ) -- ${densities(x)}\")\n",
    "})\n",
    "\n",
    "sc.parallelize(densities).coalesce(1).saveAsTextFile(OUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys.process._\n",
    "\n",
    "// Remove the old output directory\n",
    "\"rm -rf /tmp/kde\" !\n",
    "\n",
    "// Copy new output from HDFS to local filesystem\n",
    "\"hdfs dfs -copyToLocal kde /tmp/kde\" !\n",
    "\n",
    "// Generate the word cloud\n",
    "if (MODE == \"day\") {\n",
    "    \"python kde_day.py asd\" !\n",
    "}\n",
    "else {\n",
    "    \"python kde_hour.py asd\" !\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](/tmp/kde/kde.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
