{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel Density Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernel Density Estimate (KDE) is..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marking com.lucidworks.spark:spark-solr:3.6.0 for download\n",
      "-> Failed to resolve org.restlet.jee:org.restlet:2.3.0\n",
      "    -> not found: /tmp/toree-tmp-dir7627667768266578946/toree_add_deps/cache/org.restlet.jee/org.restlet/ivy-2.3.0.xml\n",
      "    -> not found: https://repo1.maven.org/maven2/org/restlet/jee/org.restlet/2.3.0/org.restlet-2.3.0.pom\n",
      "-> Failed to resolve org.restlet.jee:org.restlet.ext.servlet:2.3.0\n",
      "    -> not found: /tmp/toree-tmp-dir7627667768266578946/toree_add_deps/cache/org.restlet.jee/org.restlet.ext.servlet/ivy-2.3.0.xml\n",
      "    -> not found: https://repo1.maven.org/maven2/org/restlet/jee/org.restlet.ext.servlet/2.3.0/org.restlet.ext.servlet-2.3.0.pom\n",
      "Obtained 388 files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%AddDeps com.lucidworks.spark spark-solr 3.6.0 --transitive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Solr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "updateTime: (hour: Int, createdAt: String)Int\n",
       "timeZoneToInt: (timeZone: String)Int\n",
       "shiftHours: (hour: Int, shift: Int)Int\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def updateTime(hour:Int, createdAt:String):Int = {\n",
    "    var adjusted = hour\n",
    "    createdAt match {\n",
    "      case \"Pacific Time (US & Canada)\" => adjusted = shiftHours(hour, -8)\n",
    "      case \"Eastern Time (US & Canada)\" => adjusted = shiftHours(hour, -5)\n",
    "      case \"Central Time (US & Canada)\" => adjusted = shiftHours(hour, -5)\n",
    "      case \"Mountain Time (US & Canada)\" => adjusted = shiftHours(hour, -6)\n",
    "      case \"Atlantic Time (Canada)\" => adjusted = shiftHours(hour, -4)\n",
    "    }\n",
    "    adjusted\n",
    "}\n",
    "\n",
    "def timeZoneToInt(timeZone:String):Int = {\n",
    "    var out = 6 // sunday\n",
    "\n",
    "    if (timeZone contains \"Mon\") {\n",
    "      out = 0\n",
    "    } else if (timeZone contains \"Tue\") {\n",
    "      out = 1\n",
    "    } else if (timeZone contains \"Wed\") {\n",
    "      out = 2\n",
    "    } else if (timeZone contains \"Thu\") {\n",
    "      out = 3\n",
    "    } else if (timeZone contains \"Fri\") {\n",
    "      out = 4\n",
    "    } else if (timeZone contains \"Sat\") {\n",
    "      out = 5\n",
    "    }\n",
    "    out\n",
    "}\n",
    "\n",
    "def shiftHours(hour:Int, shift:Int):Int = {\n",
    "    var adjusted = hour + shift\n",
    "    if (adjusted >= 24) {\n",
    "      adjusted %= 24\n",
    "    } else if (adjusted < 0) {\n",
    "      adjusted += 24\n",
    "    }\n",
    "    adjusted\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SOLR = 192.168.1.111:9983\n",
       "INDEX = mb13\n",
       "MODE = day\n",
       "TERM = church\n",
       "QUERY = contents:$TERM\n",
       "LIMIT = 10000\n",
       "timeRegex = ([0-9]+):([0-9]+):([0-9]+)\n",
       "rdd = MapPartitionsRDD[33] at flatMap at <console>:95\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[33] at flatMap at <console>:95"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import com.lucidworks.spark.rdd.SelectSolrRDD\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "import org.apache.spark.mllib.stat.KernelDensity\n",
    "import play.api.libs.json._\n",
    "\n",
    "// Solr's ZooKeeper URL\n",
    "val SOLR = \"192.168.1.111:9983\"\n",
    "\n",
    "// The Solr collection\n",
    "val INDEX = \"mb13\"\n",
    "\n",
    "// The Solr query\n",
    "val MODE = \"day\"  // day OR hour\n",
    "val TERM = \"church\"\n",
    "val QUERY = \"contents:$TERM\"\n",
    "\n",
    "// The limit for number of rows to process\n",
    "val LIMIT = 1000\n",
    "\n",
    "val timeRegex = raw\"([0-9]+):([0-9]+):([0-9]+)\".r\n",
    "\n",
    "val rdd = new SelectSolrRDD(SOLR, INDEX, sc)\n",
    ".rows(1000)\n",
    ".query(QUERY)\n",
    ".flatMap(doc => {\n",
    "    val parsedJson = Json.parse(doc.get(\"raw\").toString)\n",
    "    var out:List[Tuple3[Int, Double, Int]] = List()\n",
    "\n",
    "    try {\n",
    "        val timeZone:String = (parsedJson \\ \"user\" \\ \"time_zone\").as[String]\n",
    "        if ((timeZone contains \"Canada\") || (timeZone contains \"US\")) {\n",
    "            val time = (parsedJson \\ \"created_at\").as[String]\n",
    "            val matches = timeRegex.findFirstMatchIn(time)\n",
    "            val hour = updateTime(matches.get.group(1).toInt, timeZone)\n",
    "            val week = timeZoneToInt(time)\n",
    "            val min = matches.get.group(2).toDouble\n",
    "            out = if (MODE == \"day\") List((week, hour/24, 1)) else List((hour, min/60, 1))        \n",
    "            }\n",
    "        } catch {\n",
    "          case e : Exception => println(\"unable to parse the tweet\", e)\n",
    "        }\n",
    "        out\n",
    "      }).persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute KDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: java.lang.IllegalStateException\n",
       "Message: Shard shard4 in collection mb13 does not have any active replicas!\n",
       "StackTrace:   at com.lucidworks.spark.util.SolrSupport$$anonfun$buildShardList$2$$anonfun$apply$9.apply(SolrSupport.scala:670)\n",
       "  at com.lucidworks.spark.util.SolrSupport$$anonfun$buildShardList$2$$anonfun$apply$9.apply(SolrSupport.scala:651)\n",
       "  at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)\n",
       "  at scala.collection.Iterator$class.foreach(Iterator.scala:891)\n",
       "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n",
       "  at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\n",
       "  at scala.collection.AbstractIterable.foreach(Iterable.scala:54)\n",
       "  at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)\n",
       "  at com.lucidworks.spark.util.SolrSupport$$anonfun$buildShardList$2.apply(SolrSupport.scala:651)\n",
       "  at com.lucidworks.spark.util.SolrSupport$$anonfun$buildShardList$2.apply(SolrSupport.scala:650)\n",
       "  at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n",
       "  at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n",
       "  at com.lucidworks.spark.util.SolrSupport$.buildShardList(SolrSupport.scala:650)\n",
       "  at com.lucidworks.spark.rdd.SelectSolrRDD.getPartitions(SelectSolrRDD.scala:98)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n",
       "  at scala.Option.getOrElse(Option.scala:121)\n",
       "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n",
       "  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n",
       "  at scala.Option.getOrElse(Option.scala:121)\n",
       "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n",
       "  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n",
       "  at scala.Option.getOrElse(Option.scala:121)\n",
       "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n",
       "  at org.apache.spark.Partitioner$$anonfun$4.apply(Partitioner.scala:78)\n",
       "  at org.apache.spark.Partitioner$$anonfun$4.apply(Partitioner.scala:78)\n",
       "  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "  at scala.collection.immutable.List.foreach(List.scala:392)\n",
       "  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n",
       "  at scala.collection.immutable.List.map(List.scala:296)\n",
       "  at org.apache.spark.Partitioner$.defaultPartitioner(Partitioner.scala:78)\n",
       "  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$reduceByKey$3.apply(PairRDDFunctions.scala:326)\n",
       "  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$reduceByKey$3.apply(PairRDDFunctions.scala:326)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
       "  at org.apache.spark.rdd.PairRDDFunctions.reduceByKey(PairRDDFunctions.scala:325)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val counts = rdd.map(item => (item._1, item._3)).reduceByKey(_+_).sortByKey().collect().toMap\n",
    "\n",
    "val kdeData = rdd.map(item => item._1.toInt.toDouble + item._2)\n",
    "\n",
    "val kd = if (MODE == \"day\") new KernelDensity().setSample(kdeData).setBandwidth(1.0) else new KernelDensity().setSample(kdeData).setBandwidth(2.0)\n",
    "val domain = if (MODE ==  \"day\") (0 to 6).toArray else (0 to 23).toArray\n",
    "\n",
    "val densities = kd.estimate(domain.map(_.toDouble))\n",
    "\n",
    "println(s\"counts / density per $MODE for $TERM\")\n",
    "domain.foreach(x => {\n",
    "    println(s\"$x ( ${counts(x)} ) -- ${densities(x)}\")\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
