{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named Entity Recognition (NER) the is task of extracting entities (people, organizations, locations, etc.) from natural language. In this example, we show how we can use Stanford's CoreNLP to extract names entities and build a word cloud show the relative occurrences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to import a number of dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%init_spark\n",
    "launcher.conf.spark.executor.instances = 8\n",
    "launcher.conf.spark.executor.cores = 32\n",
    "launcher.conf.spark.executor.memory = '8G'\n",
    "launcher.conf.spark.driver.memory = '4G'\n",
    "launcher.jars = [\"sparksolrini.jar\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Named Entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can extract the named entities. The output is a single file (`part-00000`) containing one entity per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import com.lucidworks.spark.rdd.SelectSolrRDD\n",
    "import edu.stanford.nlp.pipeline.{CoreDocument, StanfordCoreNLP}\n",
    "import org.apache.hadoop.fs.{FileSystem, Path}\n",
    "\n",
    "import java.util.Properties\n",
    "import collection.JavaConversions._\n",
    "\n",
    "// Solr's ZooKeeper URL\n",
    "val SOLR = \"192.168.1.111:9983\"\n",
    "\n",
    "// The Solr collection\n",
    "val INDEX = \"core17\"\n",
    "\n",
    "// The Solr query\n",
    "val QUERY = \"contents:music\"\n",
    "\n",
    "// The number of partitions\n",
    "val PARTITIONS = 8\n",
    "\n",
    "// Filter for entity type (PERSON, ORGANIZATION, LOCATION, DATE, etc.)\n",
    "val ENTITY_TYPE = \"PERSON\"\n",
    "\n",
    "// The limit for number of rows to process\n",
    "val LIMIT = 100\n",
    "\n",
    "// Output directory\n",
    "val OUT_DIR = \"ner\"\n",
    "\n",
    "// Delete old output dir\n",
    "FileSystem.get(sc.hadoopConfiguration).delete(new Path(OUT_DIR), true)\n",
    "\n",
    "val rdd = new SelectSolrRDD(SOLR, INDEX, sc, maxRows = Some(LIMIT))\n",
    "    .rows(1000)\n",
    "    .query(QUERY)\n",
    "    .repartition(PARTITIONS)\n",
    "    .mapPartitions(docs => {\n",
    "        \n",
    "        val props = new Properties()\n",
    "        props.setProperty(\"annotators\", \"tokenize,ssplit,pos,lemma,ner\")\n",
    "        props.setProperty(\"ner.applyFineGrained\", \"false\")\n",
    "        props.setProperty(\"ner.useSUTime\", \"false\")\n",
    "        props.setProperty(\"threads\", \"8\")\n",
    "        \n",
    "        val pipeline = new StanfordCoreNLP(props)\n",
    "        val entities = docs.map(doc => {\n",
    "                        \n",
    "            val coreDoc = new CoreDocument(doc.get(\"raw\").asInstanceOf[String])\n",
    "            pipeline.annotate(coreDoc)\n",
    "          \n",
    "            if (ENTITY_TYPE.equals(\"*\")) {\n",
    "                coreDoc.entityMentions().map(x => x.toString).toList\n",
    "            } else {\n",
    "                coreDoc.entityMentions().filter(cem => cem.entityType().equals(ENTITY_TYPE)).map(x => x.toString).toList\n",
    "            }\n",
    "        })\n",
    "        \n",
    "        entities\n",
    "                \n",
    "    })\n",
    "    .flatMap(x => x)\n",
    "    .coalesce(1)\n",
    "\n",
    "rdd.saveAsTextFile(OUT_DIR)\n",
    "\n",
    "rdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Word Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can generate the word cloud using the Python word_cloud package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys.process._\n",
    "\n",
    "// Remove the old output directory\n",
    "\"rm -rf /tmp/ner\" !\n",
    "\n",
    "// Copy new output from HDFS to local filesystem\n",
    "\"hdfs dfs -copyToLocal ner /tmp/ner\" !\n",
    "\n",
    "// Generate the word cloud\n",
    "\"./cloud.sh\" !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](ner.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
