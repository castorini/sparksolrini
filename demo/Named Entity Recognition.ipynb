{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to import a number of dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%AddDeps com.lucidworks.spark spark-solr 3.6.0 --transitive\n",
    "%AddDeps edu.stanford.nlp stanford-corenlp 3.9.2 --transitive\n",
    "%AddDeps edu.stanford.nlp stanford-corenlp 3.9.2 --classifier models-english"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Named Entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can extract the named entities. The output is a single file (`part-00000`) containing one entity per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import com.lucidworks.spark.rdd.SelectSolrRDD\n",
    "import edu.stanford.nlp.pipeline.{CoreDocument, StanfordCoreNLP}\n",
    "import org.apache.hadoop.fs.{FileSystem, Path}\n",
    "import java.util.Properties\n",
    "import collection.JavaConversions._\n",
    "\n",
    "// Solr's ZooKeeper URL\n",
    "val SOLR = \"localhost:9983\"\n",
    "\n",
    "// The Solr collection\n",
    "val INDEX = \"test\"\n",
    "\n",
    "// The Solr query\n",
    "val QUERY = \"*:*\"\n",
    "\n",
    "// The number of partitions\n",
    "val PARTITIONS = 8\n",
    "\n",
    "// Filter for entity type (PERSON, ORGANIZATION, LOCATION, DATE, etc.)\n",
    "val ENTITY_TYPE = \"DATE\"\n",
    "\n",
    "// Output directory\n",
    "val OUT_DIR = \"out\"\n",
    "\n",
    "// Delete old output dir\n",
    "FileSystem.get(sc.hadoopConfiguration).delete(new Path(OUT_DIR), true)\n",
    "\n",
    "val rdd = new SelectSolrRDD(SOLR, INDEX, sc)\n",
    "    .rows(1000)\n",
    "    .query(QUERY)\n",
    "    .mapPartitions(docs => {\n",
    "        \n",
    "        val props = new Properties()\n",
    "        props.setProperty(\"annotators\", \"tokenize,ssplit,pos,lemma,ner\")\n",
    "        props.setProperty(\"ner.applyFineGrained\", \"false\")\n",
    "        props.setProperty(\"ner.useSUTime\", \"false\")\n",
    "        \n",
    "        val pipeline = new StanfordCoreNLP(props)\n",
    "        val entities = docs.map(doc => {\n",
    "\n",
    "            val coreDoc = new CoreDocument(doc.get(\"contents\").asInstanceOf[String])\n",
    "            pipeline.annotate(coreDoc)\n",
    "          \n",
    "            if (ENTITY_TYPE.equals(\"*\")) {\n",
    "                coreDoc.entityMentions().toList\n",
    "            } else {\n",
    "                coreDoc.entityMentions().filter(cem => cem.entityType().equals(ENTITY_TYPE)).toList\n",
    "            }\n",
    "        })\n",
    "        \n",
    "        entities\n",
    "                \n",
    "    })\n",
    "    .flatMap(x => x)\n",
    "    .coalesce(1)\n",
    "    .saveAsTextFile(OUT_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
