2018-12-05 23:38:56 WARN  Utils:66 - Kubernetes master URL uses HTTP instead of HTTPS.
2018-12-05 23:38:56 WARN  Utils:66 - Your hostname, tem127 resolves to a loopback address: 127.0.1.1; using 192.168.152.227 instead (on interface eno1)
2018-12-05 23:38:56 WARN  Utils:66 - Set SPARK_LOCAL_IP if you need to bind to another address
2018-12-05 23:38:57 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2018-12-05 23:38:57 INFO  SolrSpark$:17 - Scallop(--term, interpol, --field, raw, --solr, 192.168.152.201:32181, --index, gov2)
 *  field => raw
 *  term => interpol
 *  solr => 192.168.152.201:32181
 *  index => gov2
    rows => 1000
    parallelism => 12
    debug => false

2018-12-05 23:38:57 INFO  SparkContext:54 - Running Spark version 2.4.0
2018-12-05 23:38:57 INFO  SparkContext:54 - Submitted application: SolrSpark$
2018-12-05 23:38:58 INFO  SecurityManager:54 - Changing view acls to: root
2018-12-05 23:38:58 INFO  SecurityManager:54 - Changing modify acls to: root
2018-12-05 23:38:58 INFO  SecurityManager:54 - Changing view acls groups to: 
2018-12-05 23:38:58 INFO  SecurityManager:54 - Changing modify acls groups to: 
2018-12-05 23:38:58 INFO  SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
2018-12-05 23:38:58 INFO  Utils:54 - Successfully started service 'sparkDriver' on port 33933.
2018-12-05 23:38:58 INFO  SparkEnv:54 - Registering MapOutputTracker
2018-12-05 23:38:58 INFO  SparkEnv:54 - Registering BlockManagerMaster
2018-12-05 23:38:58 INFO  BlockManagerMasterEndpoint:54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-12-05 23:38:58 INFO  BlockManagerMasterEndpoint:54 - BlockManagerMasterEndpoint up
2018-12-05 23:38:58 INFO  DiskBlockManager:54 - Created local directory at /tmp/blockmgr-86750510-7450-40e3-bf01-75deb7550ca6
2018-12-05 23:38:58 INFO  MemoryStore:54 - MemoryStore started with capacity 8.4 GB
2018-12-05 23:38:58 INFO  SparkEnv:54 - Registering OutputCommitCoordinator
2018-12-05 23:38:58 INFO  log:192 - Logging initialized @2988ms
2018-12-05 23:38:58 INFO  Server:351 - jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2018-12-05 23:38:58 INFO  Server:419 - Started @3088ms
2018-12-05 23:38:58 INFO  AbstractConnector:278 - Started ServerConnector@e36bb2a{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-12-05 23:38:58 INFO  Utils:54 - Successfully started service 'SparkUI' on port 4040.
2018-12-05 23:38:58 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@537b32ef{/jobs,null,AVAILABLE,@Spark}
2018-12-05 23:38:58 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6f89f665{/jobs/json,null,AVAILABLE,@Spark}
2018-12-05 23:38:58 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@df1cff6{/jobs/job,null,AVAILABLE,@Spark}
2018-12-05 23:38:58 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1ad926d3{/jobs/job/json,null,AVAILABLE,@Spark}
2018-12-05 23:38:58 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3a43d133{/stages,null,AVAILABLE,@Spark}
2018-12-05 23:38:58 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@39ce27f2{/stages/json,null,AVAILABLE,@Spark}
2018-12-05 23:38:58 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5f2afe62{/stages/stage,null,AVAILABLE,@Spark}
2018-12-05 23:38:58 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@60c16548{/stages/stage/json,null,AVAILABLE,@Spark}
2018-12-05 23:38:58 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@68105edc{/stages/pool,null,AVAILABLE,@Spark}
2018-12-05 23:38:58 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@511816c0{/stages/pool/json,null,AVAILABLE,@Spark}
2018-12-05 23:38:58 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@38b972d7{/storage,null,AVAILABLE,@Spark}
2018-12-05 23:38:58 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5339bbad{/storage/json,null,AVAILABLE,@Spark}
2018-12-05 23:38:58 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3935e9a8{/storage/rdd,null,AVAILABLE,@Spark}
2018-12-05 23:38:58 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@288a4658{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-12-05 23:38:58 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5b56b654{/environment,null,AVAILABLE,@Spark}
2018-12-05 23:38:58 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@452c8a40{/environment/json,null,AVAILABLE,@Spark}
2018-12-05 23:38:58 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@534243e4{/executors,null,AVAILABLE,@Spark}
2018-12-05 23:38:58 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@29006752{/executors/json,null,AVAILABLE,@Spark}
2018-12-05 23:38:58 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@470a9030{/executors/threadDump,null,AVAILABLE,@Spark}
2018-12-05 23:38:58 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@66d57c1b{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-12-05 23:38:58 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@27494e46{/static,null,AVAILABLE,@Spark}
2018-12-05 23:38:58 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7f9e1534{/,null,AVAILABLE,@Spark}
2018-12-05 23:38:58 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@138a7441{/api,null,AVAILABLE,@Spark}
2018-12-05 23:38:58 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7e31ce0f{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-12-05 23:38:58 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@99a65d3{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-12-05 23:38:58 INFO  SparkUI:54 - Bound SparkUI to 0.0.0.0, and started at http://192.168.152.227:4040
2018-12-05 23:38:58 INFO  SparkContext:54 - Added JAR file:/hdd1/CS848-project/target/cs848-project-1.0-SNAPSHOT.jar at spark://192.168.152.227:33933/jars/cs848-project-1.0-SNAPSHOT.jar with timestamp 1544071138860
2018-12-05 23:38:58 WARN  Config:347 - Error reading service account token from: [/var/run/secrets/kubernetes.io/serviceaccount/token]. Ignoring.
2018-12-05 23:38:59 INFO  ExecutorPodsAllocator:54 - Going to request 5 executors from Kubernetes.
2018-12-05 23:38:59 INFO  Utils:54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41193.
2018-12-05 23:38:59 INFO  NettyBlockTransferService:54 - Server created on 192.168.152.227:41193
2018-12-05 23:38:59 INFO  BlockManager:54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-12-05 23:38:59 INFO  BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, 192.168.152.227, 41193, None)
2018-12-05 23:38:59 INFO  BlockManagerMasterEndpoint:54 - Registering block manager 192.168.152.227:41193 with 8.4 GB RAM, BlockManagerId(driver, 192.168.152.227, 41193, None)
2018-12-05 23:38:59 INFO  BlockManagerMaster:54 - Registered BlockManager BlockManagerId(driver, 192.168.152.227, 41193, None)
2018-12-05 23:38:59 INFO  BlockManager:54 - Initialized BlockManager: BlockManagerId(driver, 192.168.152.227, 41193, None)
2018-12-05 23:38:59 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@bd1111a{/metrics/json,null,AVAILABLE,@Spark}
2018-12-05 23:39:13 INFO  KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint:54 - Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.152.201:56138) with ID 2
2018-12-05 23:39:14 INFO  KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint:54 - Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.152.203:50486) with ID 1
2018-12-05 23:39:14 INFO  BlockManagerMasterEndpoint:54 - Registering block manager 10.233.102.144:44943 with 4.4 GB RAM, BlockManagerId(2, 10.233.102.144, 44943, None)
2018-12-05 23:39:14 INFO  BlockManagerMasterEndpoint:54 - Registering block manager 10.233.71.63:43669 with 4.4 GB RAM, BlockManagerId(1, 10.233.71.63, 43669, None)
2018-12-05 23:39:14 INFO  KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint:54 - Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.152.204:52428) with ID 5
2018-12-05 23:39:14 INFO  BlockManagerMasterEndpoint:54 - Registering block manager 10.233.74.79:33201 with 4.4 GB RAM, BlockManagerId(5, 10.233.74.79, 33201, None)
2018-12-05 23:39:15 INFO  KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint:54 - Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.152.205:52374) with ID 3
2018-12-05 23:39:15 INFO  KubernetesClusterSchedulerBackend:54 - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
2018-12-05 23:39:16 INFO  BlockManagerMasterEndpoint:54 - Registering block manager 10.233.97.183:41217 with 4.4 GB RAM, BlockManagerId(3, 10.233.97.183, 41217, None)
2018-12-05 23:39:16 INFO  ZooKeeper:100 - Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT
2018-12-05 23:39:16 INFO  ZooKeeper:100 - Client environment:host.name=tem127.tembo-domain.cs.uwaterloo.ca
2018-12-05 23:39:16 INFO  ZooKeeper:100 - Client environment:java.version=1.8.0_181
2018-12-05 23:39:16 INFO  ZooKeeper:100 - Client environment:java.vendor=Oracle Corporation
2018-12-05 23:39:16 INFO  ZooKeeper:100 - Client environment:java.home=/usr/lib/jvm/java-8-openjdk-amd64/jre
2018-12-05 23:39:16 INFO  ZooKeeper:100 - Client environment:java.class.path=/home/j474lee/spark-2.4.0-bin-hadoop2.7/conf/:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/calcite-avatica-1.2.0-incubating.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/slf4j-api-1.7.16.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/snappy-0.2.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/machinist_2.11-0.6.1.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/hadoop-yarn-common-2.7.3.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/opencsv-2.3.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/datanucleus-core-3.2.10.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/spark-mllib-local_2.11-2.4.0.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/hadoop-hdfs-2.7.3.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/paranamer-2.8.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/hadoop-auth-2.7.3.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/jackson-module-scala_2.11-2.6.7.1.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/spark-unsafe_2.11-2.4.0.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/commons-beanutils-1.7.0.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/orc-core-1.5.2-nohive.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/commons-lang3-3.5.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/jersey-server-2.22.2.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/commons-compiler-3.0.9.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/jackson-annotations-2.6.7.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/kubernetes-client-3.0.0.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/log4j-1.2.17.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/ST4-4.0.4.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/janino-3.0.9.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/hadoop-mapreduce-client-app-2.7.3.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/JavaEWAH-0.3.2.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/scala-parser-combinators_2.11-1.1.0.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/commons-configuration-1.6.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/spark-tags_2.11-2.4.0.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/xmlenc-0.52.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/commons-io-2.4.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/lz4-java-1.4.0.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/antlr-runtime-3.4.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/validation-api-1.1.0.Final.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/zstd-jni-1.3.2-2.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/api-util-1.0.0-M20.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/datanucleus-rdbms-3.2.9.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/javax.servlet-api-3.1.0.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/scala-reflect-2.11.12.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/jackson-jaxrs-1.9.13.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/libthrift-0.9.3.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/jetty-6.1.26.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/commons-pool-1.5.4.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/datanucleus-api-jdo-3.2.6.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/commons-logging-1.1.3.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/stringtemplate-3.2.1.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/apacheds-i18n-2.0.0-M15.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/jaxb-api-2.2.2.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/spark-repl_2.11-2.4.0.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/stax-api-1.0.1.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/jersey-guava-2.22.2.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/core-1.1.2.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/breeze_2.11-0.13.2.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/logging-interceptor-3.8.1.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/hadoop-annotations-2.7.3.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/hadoop-yarn-client-2.7.3.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/hive-exec-1.2.1.spark2.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/json4s-core_2.11-3.5.3.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/spark-tags_2.11-2.4.0-tests.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/jdo-api-3.0.1.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/parquet-hadoop-1.10.0.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/hadoop-yarn-server-common-2.7.3.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/javax.ws.rs-api-2.0.1.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/spark-graphx_2.11-2.4.0.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/hppc-0.7.2.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/metrics-jvm-3.1.5.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/hadoop-mapreduce-client-core-2.7.3.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/spark-sketch_2.11-2.4.0.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/httpcore-4.4.10.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/py4j-0.10.7.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/commons-net-3.1.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/xercesImpl-2.9.1.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/hadoop-mapreduce-client-common-2.7.3.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/arpack_combined_all-0.1.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/aircompressor-0.10.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/spark-launcher_2.11-2.4.0.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/automaton-1.11-8.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/curator-client-2.7.1.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/spark-hive-thriftserver_2.11-2.4.0.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/hadoop-mapreduce-client-jobclient-2.7.3.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/snakeyaml-1.15.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/hk2-utils-2.4.0-b34.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/jackson-dataformat-yaml-2.6.7.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/metrics-json-3.1.5.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/guava-14.0.1.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/avro-ipc-1.8.2.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/bonecp-0.8.0.RELEASE.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/jersey-container-servlet-2.22.2.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/hadoop-yarn-server-web-proxy-2.7.3.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/metrics-core-3.1.5.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/jersey-container-servlet-core-2.22.2.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/spark-yarn_2.11-2.4.0.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/api-asn1-api-1.0.0-M20.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/spark-mesos_2.11-2.4.0.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/jline-2.14.6.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/spire-macros_2.11-0.13.0.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/jersey-client-2.22.2.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/ivy-2.4.0.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/pyrolite-4.13.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/xz-1.5.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/commons-crypto-1.0.0.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/zookeeper-3.4.6.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/javassist-3.18.1-GA.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/hk2-locator-2.4.0-b34.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/spark-hive_2.11-2.4.0.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/jodd-core-3.5.2.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/calcite-linq4j-1.2.0-incubating.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/avro-1.8.2.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/stax-api-1.0-2.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/chill_2.11-0.9.3.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/commons-beanutils-core-1.8.0.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/javolution-5.5.1.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/okhttp-3.8.1.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/commons-math3-3.4.1.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/libfb303-0.9.3.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/parquet-jackson-1.10.0.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/spark-catalyst_2.11-2.4.0.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/jackson-core-asl-1.9.13.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/spark-kvstore_2.11-2.4.0.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/guice-3.0.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/jackson-mapper-asl-1.9.13.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/commons-dbcp-1.4.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/jpam-1.1.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/hadoop-client-2.7.3.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/hk2-api-2.4.0-b34.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/avro-mapred-1.8.2-hadoop2.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/json4s-jackson_2.11-3.5.3.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/breeze-macros_2.11-0.13.2.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/eigenbase-properties-1.1.5.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/RoaringBitmap-0.5.11.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/shapeless_2.11-2.3.2.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/hadoop-yarn-api-2.7.3.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/activation-1.1.1.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/commons-cli-1.2.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/jersey-common-2.22.2.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/curator-recipes-2.7.1.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/httpclient-4.5.6.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/snappy-java-1.1.7.1.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/spark-sql_2.11-2.4.0.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/arrow-vector-0.10.0.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/protobuf-java-2.5.0.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/jackson-xc-1.9.13.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/jsp-api-2.1.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/apacheds-kerberos-codec-2.0.0-M15.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/jta-1.1.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/hadoop-common-2.7.3.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/jcl-over-slf4j-1.7.16.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/jackson-module-paranamer-2.7.9.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/jackson-databind-2.6.7.1.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/hive-beeline-1.2.1.spark2.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/jetty-util-6.1.26.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/spark-streaming_2.11-2.4.0.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/jackson-module-jaxb-annotations-2.6.7.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/spark-core_2.11-2.4.0.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/leveldbjni-all-1.8.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/guice-servlet-3.0.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/mesos-1.4.0-shaded-protobuf.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/scala-library-2.11.12.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/parquet-column-1.10.0.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/objenesis-2.5.1.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/chill-java-0.9.3.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/arrow-format-0.10.0.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/apache-log4j-extras-1.2.17.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/json4s-ast_2.11-3.5.3.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/antlr4-runtime-4.7.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/json4s-scalap_2.11-3.5.3.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/gson-2.2.4.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/hadoop-mapreduce-client-shuffle-2.7.3.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/arrow-memory-0.10.0.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/spark-kubernetes_2.11-2.4.0.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/macro-compat_2.11-1.1.1.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/scala-compiler-2.11.12.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/orc-mapreduce-1.5.2-nohive.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/jsr305-1.3.9.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/aopalliance-repackaged-2.4.0-b34.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/commons-digester-1.8.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/stream-2.7.0.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/javax.inject-1.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/xbean-asm6-shaded-4.8.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/orc-shims-1.5.2.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/antlr-2.7.7.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/parquet-encoding-1.10.0.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/spark-mllib_2.11-2.4.0.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/jtransforms-2.4.0.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/hive-metastore-1.2.1.spark2.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/derby-10.12.1.1.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/htrace-core-3.1.0-incubating.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/parquet-common-1.10.0.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/kubernetes-model-2.0.0.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/metrics-graphite-3.1.5.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/parquet-format-2.4.0.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/compress-lzf-1.0.3.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/zjsonpatch-0.3.0.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/minlog-1.3.0.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/slf4j-log4j12-1.7.16.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/flatbuffers-1.2.0-3f79e055.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/generex-1.0.1.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/commons-compress-1.8.1.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/netty-3.9.9.Final.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/okio-1.13.0.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/super-csv-2.2.0.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/joda-time-2.9.3.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/javax.inject-2.4.0-b34.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/aopalliance-1.0.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/curator-framework-2.7.1.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/commons-collections-3.2.2.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/spark-network-shuffle_2.11-2.4.0.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/netty-all-4.1.17.Final.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/osgi-resource-locator-1.0.1.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/jul-to-slf4j-1.7.16.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/calcite-core-1.2.0-incubating.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/spire_2.11-0.13.0.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/commons-httpclient-3.1.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/jackson-core-2.6.7.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/javax.annotation-api-1.2.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/oro-2.0.8.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/univocity-parsers-2.7.3.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/kryo-shaded-4.0.2.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/commons-lang-2.6.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/parquet-hadoop-bundle-1.6.0.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/hive-cli-1.2.1.spark2.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/scala-xml_2.11-1.0.5.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/spark-network-common_2.11-2.4.0.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/jersey-media-jaxb-2.22.2.jar:/home/j474lee/spark-2.4.0-bin-hadoop2.7/jars/commons-codec-1.10.jar:/home/j474lee/hdfs_conf/
2018-12-05 23:39:16 INFO  ZooKeeper:100 - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib
2018-12-05 23:39:16 INFO  ZooKeeper:100 - Client environment:java.io.tmpdir=/tmp
2018-12-05 23:39:16 INFO  ZooKeeper:100 - Client environment:java.compiler=<NA>
2018-12-05 23:39:16 INFO  ZooKeeper:100 - Client environment:os.name=Linux
2018-12-05 23:39:16 INFO  ZooKeeper:100 - Client environment:os.arch=amd64
2018-12-05 23:39:16 INFO  ZooKeeper:100 - Client environment:os.version=4.15.0-33-generic
2018-12-05 23:39:16 INFO  ZooKeeper:100 - Client environment:user.name=root
2018-12-05 23:39:16 INFO  ZooKeeper:100 - Client environment:user.home=/root
2018-12-05 23:39:16 INFO  ZooKeeper:100 - Client environment:user.dir=/home/j474lee
2018-12-05 23:39:16 INFO  ZooKeeper:438 - Initiating client connection, connectString=192.168.152.201:32181 sessionTimeout=30000 watcher=org.apache.solr.common.cloud.SolrZkClient$1@7569ea63
2018-12-05 23:39:16 INFO  ClientCnxn:975 - Opening socket connection to server 192.168.152.201/192.168.152.201:32181. Will not attempt to authenticate using SASL (unknown error)
2018-12-05 23:39:16 INFO  ClientCnxn:852 - Socket connection established to 192.168.152.201/192.168.152.201:32181, initiating session
2018-12-05 23:39:16 INFO  ClientCnxn:1235 - Session establishment complete on server 192.168.152.201/192.168.152.201:32181, sessionid = 0x36762080b9a0061, negotiated timeout = 30000
2018-12-05 23:39:16 INFO  ConnectionManager:119 - zkClient has connected
2018-12-05 23:39:16 INFO  ZkStateReader:786 - Updated live nodes from ZooKeeper... (0) -> (5)
2018-12-05 23:39:16 INFO  ZkClientClusterStateProvider:158 - Cluster at 192.168.152.201:32181 ready
2018-12-05 23:39:16 INFO  SelectSolrRDD:8 - Updated Solr query: q=raw:interpol&rows=1000&collection=gov2&distrib=false&start=0&sort=id+asc&fq=_version_:[*+TO+1618535134488690688]
2018-12-05 23:39:16 INFO  SelectSolrRDD:8 - Found 10 partitions
2018-12-05 23:39:16 INFO  SparkContext:54 - Starting job: count at SolrSpark.scala:31
2018-12-05 23:39:16 INFO  DAGScheduler:54 - Got job 0 (count at SolrSpark.scala:31) with 10 output partitions
2018-12-05 23:39:16 INFO  DAGScheduler:54 - Final stage: ResultStage 0 (count at SolrSpark.scala:31)
2018-12-05 23:39:16 INFO  DAGScheduler:54 - Parents of final stage: List()
2018-12-05 23:39:16 INFO  DAGScheduler:54 - Missing parents: List()
2018-12-05 23:39:16 INFO  DAGScheduler:54 - Submitting ResultStage 0 (SelectSolrRDD[2] at RDD at SolrRDD.scala:26), which has no missing parents
2018-12-05 23:39:16 INFO  MemoryStore:54 - Block broadcast_0 stored as values in memory (estimated size 2.1 KB, free 8.4 GB)
2018-12-05 23:39:16 INFO  MemoryStore:54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 1529.0 B, free 8.4 GB)
2018-12-05 23:39:16 INFO  BlockManagerInfo:54 - Added broadcast_0_piece0 in memory on 192.168.152.227:41193 (size: 1529.0 B, free: 8.4 GB)
2018-12-05 23:39:16 INFO  SparkContext:54 - Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2018-12-05 23:39:16 INFO  DAGScheduler:54 - Submitting 10 missing tasks from ResultStage 0 (SelectSolrRDD[2] at RDD at SolrRDD.scala:26) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
2018-12-05 23:39:16 INFO  TaskSchedulerImpl:54 - Adding task set 0.0 with 10 tasks
2018-12-05 23:39:16 INFO  TaskSetManager:54 - Starting task 0.0 in stage 0.0 (TID 0, 10.233.102.144, executor 2, partition 0, ANY, 9176 bytes)
2018-12-05 23:39:16 INFO  TaskSetManager:54 - Starting task 1.0 in stage 0.0 (TID 1, 10.233.71.63, executor 1, partition 1, ANY, 9176 bytes)
2018-12-05 23:39:16 INFO  TaskSetManager:54 - Starting task 2.0 in stage 0.0 (TID 2, 10.233.74.79, executor 5, partition 2, ANY, 9176 bytes)
2018-12-05 23:39:16 INFO  TaskSetManager:54 - Starting task 3.0 in stage 0.0 (TID 3, 10.233.97.183, executor 3, partition 3, ANY, 9176 bytes)
2018-12-05 23:39:16 INFO  TaskSetManager:54 - Starting task 4.0 in stage 0.0 (TID 4, 10.233.102.144, executor 2, partition 4, ANY, 9176 bytes)
2018-12-05 23:39:16 INFO  TaskSetManager:54 - Starting task 5.0 in stage 0.0 (TID 5, 10.233.71.63, executor 1, partition 5, ANY, 9176 bytes)
2018-12-05 23:39:16 INFO  TaskSetManager:54 - Starting task 6.0 in stage 0.0 (TID 6, 10.233.74.79, executor 5, partition 6, ANY, 9176 bytes)
2018-12-05 23:39:16 INFO  TaskSetManager:54 - Starting task 7.0 in stage 0.0 (TID 7, 10.233.97.183, executor 3, partition 7, ANY, 9176 bytes)
2018-12-05 23:39:16 INFO  TaskSetManager:54 - Starting task 8.0 in stage 0.0 (TID 8, 10.233.102.144, executor 2, partition 8, ANY, 9176 bytes)
2018-12-05 23:39:16 INFO  TaskSetManager:54 - Starting task 9.0 in stage 0.0 (TID 9, 10.233.71.63, executor 1, partition 9, ANY, 9176 bytes)
2018-12-05 23:39:18 INFO  KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint:54 - Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.152.202:52106) with ID 4
2018-12-05 23:39:18 INFO  BlockManagerMasterEndpoint:54 - Registering block manager 10.233.75.32:36637 with 4.4 GB RAM, BlockManagerId(4, 10.233.75.32, 36637, None)
2018-12-05 23:39:26 INFO  BlockManagerInfo:54 - Added broadcast_0_piece0 in memory on 10.233.74.79:33201 (size: 1529.0 B, free: 4.4 GB)
2018-12-05 23:39:26 INFO  BlockManagerInfo:54 - Added broadcast_0_piece0 in memory on 10.233.71.63:43669 (size: 1529.0 B, free: 4.4 GB)
2018-12-05 23:39:26 INFO  BlockManagerInfo:54 - Added broadcast_0_piece0 in memory on 10.233.97.183:41217 (size: 1529.0 B, free: 4.4 GB)
2018-12-05 23:39:26 INFO  BlockManagerInfo:54 - Added broadcast_0_piece0 in memory on 10.233.102.144:44943 (size: 1529.0 B, free: 4.4 GB)
2018-12-05 23:40:30 INFO  TaskSetManager:54 - Finished task 4.0 in stage 0.0 (TID 4) in 73843 ms on 10.233.102.144 (executor 2) (1/10)
2018-12-05 23:40:31 INFO  TaskSetManager:54 - Finished task 5.0 in stage 0.0 (TID 5) in 74263 ms on 10.233.71.63 (executor 1) (2/10)
2018-12-05 23:40:47 INFO  TaskSetManager:54 - Finished task 7.0 in stage 0.0 (TID 7) in 90815 ms on 10.233.97.183 (executor 3) (3/10)
2018-12-05 23:40:47 INFO  TaskSetManager:54 - Finished task 6.0 in stage 0.0 (TID 6) in 90849 ms on 10.233.74.79 (executor 5) (4/10)
2018-12-05 23:41:10 INFO  TaskSetManager:54 - Finished task 3.0 in stage 0.0 (TID 3) in 113630 ms on 10.233.97.183 (executor 3) (5/10)
2018-12-05 23:41:10 INFO  TaskSetManager:54 - Finished task 2.0 in stage 0.0 (TID 2) in 113676 ms on 10.233.74.79 (executor 5) (6/10)
2018-12-05 23:41:16 INFO  TaskSetManager:54 - Finished task 0.0 in stage 0.0 (TID 0) in 119744 ms on 10.233.102.144 (executor 2) (7/10)
2018-12-05 23:41:16 INFO  TaskSetManager:54 - Finished task 1.0 in stage 0.0 (TID 1) in 119801 ms on 10.233.71.63 (executor 1) (8/10)
2018-12-05 23:41:44 INFO  TaskSetManager:54 - Finished task 8.0 in stage 0.0 (TID 8) in 148033 ms on 10.233.102.144 (executor 2) (9/10)
2018-12-05 23:41:45 INFO  TaskSetManager:54 - Finished task 9.0 in stage 0.0 (TID 9) in 148547 ms on 10.233.71.63 (executor 1) (10/10)
2018-12-05 23:41:45 INFO  DAGScheduler:54 - ResultStage 0 (count at SolrSpark.scala:31) finished in 148.803 s
2018-12-05 23:41:45 INFO  TaskSchedulerImpl:54 - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2018-12-05 23:41:45 INFO  DAGScheduler:54 - Job 0 finished: count at SolrSpark.scala:31, took 148.899268 s
2018-12-05 23:41:45 INFO  SolrSpark$:33 - Took 149624ms
2018-12-05 23:41:45 INFO  ClientCnxn:512 - EventThread shut down
2018-12-05 23:41:45 INFO  ZooKeeper:684 - Session: 0x36762080b9a0061 closed
2018-12-05 23:41:45 INFO  AbstractConnector:318 - Stopped Spark@e36bb2a{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-12-05 23:41:45 INFO  SparkUI:54 - Stopped Spark web UI at http://192.168.152.227:4040
2018-12-05 23:41:45 INFO  KubernetesClusterSchedulerBackend:54 - Shutting down all executors
2018-12-05 23:41:45 INFO  KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint:54 - Asking each executor to shut down
2018-12-05 23:41:45 WARN  ExecutorPodsWatchSnapshotSource:87 - Kubernetes client has been closed (this is expected if the application is shutting down.)
2018-12-05 23:41:46 INFO  MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!
2018-12-05 23:41:46 INFO  MemoryStore:54 - MemoryStore cleared
2018-12-05 23:41:46 INFO  BlockManager:54 - BlockManager stopped
2018-12-05 23:41:46 INFO  BlockManagerMaster:54 - BlockManagerMaster stopped
2018-12-05 23:41:46 INFO  OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!
2018-12-05 23:41:46 INFO  SparkContext:54 - Successfully stopped SparkContext
2018-12-05 23:41:46 INFO  ShutdownHookManager:54 - Shutdown hook called
2018-12-05 23:41:46 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-c42b5821-8cc0-4e56-856e-9f24d536a854
2018-12-05 23:41:46 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-b8385478-4f44-48b8-800a-690d51d70eb4

